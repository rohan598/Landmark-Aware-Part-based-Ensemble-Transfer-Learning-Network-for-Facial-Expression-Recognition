{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summary_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -i https://test.pypi.org/simple/ monitor-training==0.1.3"
      ],
      "metadata": {
        "id": "wLVMEMY1qM9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjLqVR9q2FHP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "from tensorboard import notebook\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import dlib\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Concatenate, Dropout, Activation, Add, Input, Dense, Conv2D, Flatten, GlobalAvgPool2D, GlobalMaxPool2D, MaxPooling2D, AvgPool2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from monitor_training import MonitorSingleTraining"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HswfTdtZKd7R"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc28y1R62FHW"
      },
      "source": [
        "# Global variables\n",
        "- Use models/checkpoint/ folder for storing checkpoints\n",
        "- Use models/recovery/ folder for storing recovery json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2oCW0Tu2FHX"
      },
      "source": [
        "# class label dictionaries\n",
        "CLASS_LABEL_MAP = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprise\"}\n",
        "LABEL_CLASS_MAP = {\"Angry\":0, \"Disgust\":1, \"Fear\":2, \"Happy\":3, \"Neutral\":4, \"Sad\":5, \"Surprise\":6}\n",
        "\n",
        "# network input width and height\n",
        "IMG_WIDTH = 160\n",
        "IMG_HEIGHT = 160\n",
        "\n",
        "SEED_VALUE = 41\n",
        "random.seed(SEED_VALUE)\n",
        "\n",
        "CHECKPOINT_PATH = \"../models/checkpoint/your dataset name\"\n",
        "RECOVERY_DIR = \"../models/recovery/your dataset name\"\n",
        "drive_data_path = \"../data/your dataset folder\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkBGySiFoWjZ"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVCsMwhL2FIl"
      },
      "source": [
        "# Create Facial Landmark Annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feFHxIpcK6Il"
      },
      "source": [
        "## OPENCV FACE DETECTORS\n",
        "- Use models/utils/ folder to store face and landmark detection models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uY1jSEUAPYa"
      },
      "source": [
        "## opencv DNN caffe model\n",
        "opencv_dnn_caffe_model = \"../models/utils/path to .caffemodel\"\n",
        "opencv_dnn_caffe_prototxt = \"../models/utils/path to .prototxt\"\n",
        "caffe_net = cv2.dnn.readNetFromCaffe(opencv_dnn_caffe_prototxt, opencv_dnn_caffe_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI9znH5jK9IE"
      },
      "source": [
        "## DLIB LANDMARKS PREDICTOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3mTLmuGSwxp"
      },
      "source": [
        "### DLIB LANDMARKS PREDICTOR : ERT ALGORITHM\n",
        "ert_landmarks_predictor = dlib.shape_predictor(\"../models/utils/path to .dat\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWig3P0QAbfE"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAUB_VB8Cd0"
      },
      "source": [
        "def rect_to_bb_hog(face):\n",
        "\t# take a bounding predicted by dlib and convert it\n",
        "\t# to the format (x, y, w, h) as we would normally do\n",
        "\t# with OpenCV\n",
        "\tx = face.left()\n",
        "\ty = face.top()\n",
        "\tw = face.right() - x\n",
        "\th = face.bottom() - y\n",
        "\t# return a tuple of (x, y, w, h)\n",
        "\treturn (x, y, w, h)\n",
        "\n",
        "def rect_to_bb_cnn(face):\n",
        "\t# take a bounding predicted by dlib and convert it\n",
        "\t# to the format (x, y, w, h) as we would normally do\n",
        "\t# with OpenCV\n",
        "\tx = face.rect.left()\n",
        "\ty = face.rect.top()\n",
        "\tw = face.rect.right() - x\n",
        "\th = face.rect.bottom() - y\n",
        "\th = int(h*1.5)\n",
        "\n",
        "\t# return a tuple of (x, y, w, h)\n",
        "\treturn (x, y, w, h)\n",
        "\n",
        "def landmarks_to_np(shape, dtype=\"int\"):\n",
        "\t# initialize the list of (x, y)-coordinates\n",
        "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
        "\t# loop over the 68 facial landmarks and convert them\n",
        "\t# to a 2-tuple of (x, y)-coordinates\n",
        "\tfor i in range(0, 68):\n",
        "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
        "\t# return the list of (x, y)-coordinates\n",
        "\treturn coords\n",
        "\n",
        "def visualize_face(im, x, y, w, h, landmarks):\n",
        "\t# draw box over face\n",
        "\tcv2.rectangle(im, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        " \n",
        "\t# # loop over the (x, y)-coordinates for the facial landmarks and draw them on the image\n",
        "\tfor (xl, yl) in landmarks.reshape((int(landmarks.shape[0]/2),2)):\n",
        "\t\tcv2.circle(im,(xl, yl), 1, (0, 0, 255), -1)\n",
        "\t\n",
        "\tplt.imshow(im)\n",
        "\tplt.show()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv-PtyAyY_NY"
      },
      "source": [
        "# Load img/label data to drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_face_crop, valid_face_crop, y_train, y_valid = [], [], [], []\n",
        "# face crops of size 160x160 are fed to model\n",
        "# use the pre-processing pipeline mentioned in the paper \n",
        "# to get face crops for train and validation (or cross-validation) portions of dataset"
      ],
      "metadata": {
        "id": "4LGE3za73qcS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HXzJefuEYXb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCKN14dzduRq"
      },
      "source": [
        "## Custom Data Generator for model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLR80wUoKIxb"
      },
      "source": [
        "class ImageDataGeneratorLandmarksOnly(object):\n",
        "  def __init__(self,\n",
        "               datagen,\n",
        "               landmarks_predictor,\n",
        "               target_shape=None,\n",
        "               gen_batch_size = 1, \n",
        "               preprocess_input = None,\n",
        "               start_idx = 0,\n",
        "               end_idx = 68,\n",
        "               verbose = 0):\n",
        "      \n",
        "    self.datagen = datagen\n",
        "    self.target_shape = target_shape\n",
        "    self.landmarks_predictor = landmarks_predictor\n",
        "    print(type(preprocess_input))\n",
        "\n",
        "    self.preprocess_input = preprocess_input\n",
        "    self.gen_batch_size = gen_batch_size\n",
        "    self.start_idx = start_idx\n",
        "    self.end_idx = end_idx\n",
        "\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def flow(self, X, y, batch_size = 4):\n",
        "\n",
        "    generator = self.datagen.flow(X, y, batch_size = self.gen_batch_size)\n",
        "\n",
        "    while True:\n",
        "\n",
        "      N = 0\n",
        "      X_batch, ylr_batch = [], []\n",
        "      dict_labels_collected = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}\n",
        "      got2 = False\n",
        "\n",
        "      while N < batch_size:\n",
        "        X_gen, yemotion_gen = generator.next()\n",
        "\n",
        "        skip_cnt = 0\n",
        "        for i in range(X_gen.shape[0]):\n",
        "\n",
        "          if got2==True and dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]>=(batch_size/7):\n",
        "            skip_cnt+=1\n",
        "            continue\n",
        "\n",
        "          Xaug = X_gen[i]\n",
        "          Xaug = Xaug.astype(dtype='uint8')\n",
        "\n",
        "          x1, y1, x2, y2,w,h = 0,0,160,160,160,160\n",
        "          face_rect = dlib.rectangle(x1, y1, x2, y2)\n",
        "          landmarks = self.landmarks_predictor(Xaug, face_rect) \n",
        "          landmarks = landmarks_to_np(landmarks)\n",
        "\n",
        "          if self.verbose == 1:\n",
        "            visualize_face(np.array(Xaug),x1,y1,w,h,landmarks)\n",
        "     \n",
        "          skip_image = False\n",
        "\n",
        "          landmarks = landmarks.astype(np.float32)\n",
        "\n",
        "          for j in range(landmarks.shape[0]):\n",
        "            landmarks[j][0]/=(IMG_WIDTH)\n",
        "            landmarks[j][1]/=(IMG_HEIGHT)\n",
        "\n",
        "          landmarks = landmarks.reshape((-1,))\n",
        "\n",
        "          if skip_image==False:\n",
        "            Xaug = Xaug.astype(dtype='float32')\n",
        "            X_batch.append(Xaug)\n",
        "            ylr_batch.append(landmarks[2*self.start_idx:2*self.end_idx])\n",
        "            dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]+=1\n",
        "            if dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]==((batch_size/7)+1):\n",
        "              got2=True\n",
        "          \n",
        "          else:\n",
        "            skip_cnt += 1 \n",
        "        \n",
        "          if len(X_batch)>=batch_size:\n",
        "            break\n",
        "\n",
        "        N+=self.gen_batch_size - skip_cnt\n",
        "\n",
        "      X_batch, ylr_batch = np.array(X_batch), np.array(ylr_batch)\n",
        "      X_batch =self.preprocess_input(X_batch)\n",
        "      \n",
        "      yield (X_batch,ylr_batch)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQwcGWQBw4rs"
      },
      "source": [
        "class ImageDataGeneratorEmotionOnly(object):\n",
        "  def __init__(self,\n",
        "               datagen,\n",
        "               landmarks_predictor,\n",
        "               target_shape=None,\n",
        "               gen_batch_size = 1,\n",
        "               preprocess_input = None,\n",
        "               verbose = 0):\n",
        "      \n",
        "    self.datagen = datagen\n",
        "    self.target_shape = target_shape\n",
        "    self.landmarks_predictor = landmarks_predictor\n",
        "    self.gen_batch_size = gen_batch_size\n",
        "    self.preprocess_input = preprocess_input\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def flow(self, X, y, batch_size = 4):\n",
        "\n",
        "    generator = self.datagen.flow(X, y, batch_size = batch_size)\n",
        "    \n",
        "    while True:\n",
        "\n",
        "      N = 0\n",
        "      X_batch, yemotion_batch = [], []\n",
        "      dict_labels_collected = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}\n",
        "      got2 = False\n",
        "  \n",
        "      while N < batch_size:\n",
        "        X_gen, yemotion_gen = generator.next()\n",
        "\n",
        "        skip_cnt = 0\n",
        "        for i in range(X_gen.shape[0]):\n",
        "          \n",
        "          if got2==True and dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]>=(batch_size/7):\n",
        "            skip_cnt+=1\n",
        "            continue\n",
        "          \n",
        "          Xaug = X_gen[i]\n",
        "          Xaug = Xaug.astype(dtype='uint8')\n",
        "          \n",
        "          x1, y1, x2, y2,w,h = 0,0,160,160,160,160\n",
        "          face_rect = dlib.rectangle(x1, y1, x2, y2)\n",
        "          landmarks = self.landmarks_predictor(Xaug, face_rect) \n",
        "          landmarks = landmarks_to_np(landmarks)\n",
        "\n",
        "          if self.verbose == 1:\n",
        "            visualize_face(np.array(Xaug),x1,y1,w,h,landmarks)\n",
        "\n",
        "          skip_image = False\n",
        "\n",
        "          if skip_image==False:\n",
        "            Xaug = Xaug.astype(dtype='float32')\n",
        "            X_batch.append(Xaug)\n",
        "            \n",
        "            yemotion_batch.append(yemotion_gen[i])\n",
        "            dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]+=1\n",
        "            if dict_labels_collected[np.where(yemotion_gen[i]==1)[0][0]]==((batch_size/7)+1):\n",
        "              got2=True\n",
        "            \n",
        "          \n",
        "          else:\n",
        "            skip_cnt += 1 \n",
        "          \n",
        "          if len(X_batch)>=batch_size:\n",
        "            break\n",
        "\n",
        "        N+=batch_size - skip_cnt\n",
        "\n",
        "      X_batch, yemotion_batch= np.array(X_batch), np.array(yemotion_batch)\n",
        "      X_batch =self.preprocess_input(X_batch) \n",
        "\n",
        "      yield (X_batch,yemotion_batch)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRsXb9e5Z87B"
      },
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrA9MBhxZ8AW"
      },
      "source": [
        "def preprocess_input_v1(x):\n",
        "  x /= 255.0\n",
        "  x -= 0.5\n",
        "  x *= 2.0\n",
        "  return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWN8JyFJ5Lj"
      },
      "source": [
        "### Create data augmentors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfKIViCEmsvS"
      },
      "source": [
        "aug = ImageDataGenerator(\n",
        "\t\trotation_range=15,\n",
        "\t\twidth_shift_range=0.05,\n",
        "\t\theight_shift_range=0.05,\n",
        "\t\tshear_range=0.05, \n",
        "\t\thorizontal_flip=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpANlVJvOOtm"
      },
      "source": [
        "## Create Test dataset for landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00D82-Et2NLX"
      },
      "source": [
        "def create_test_dataset(X, landmarks_predictor, preprocess_input = None, start_idx = 0, end_idx = 68, verbose=0):\n",
        "  \n",
        "  Xlr, ylr = [], []\n",
        "  for i in range(X.shape[0]):\n",
        "    x1, y1, x2, y2,w,h = 0,0,160,160,160,160\n",
        "    face_rect = dlib.rectangle(x1, y1, x2, y2)\n",
        "    landmarks = landmarks_predictor(X[i], face_rect) \n",
        "    landmarks = landmarks_to_np(landmarks)\n",
        "    landmarks = landmarks.astype(dtype='float32')\n",
        "\n",
        "    landmarks = landmarks.reshape((-1,))\n",
        "\n",
        "    if verbose==1:\n",
        "      visualize_face(np.array(X[i]),x1,y1,int(w),int(h),landmarks[2*start_idx:2*end_idx])\n",
        "\n",
        "    landmarks = landmarks/IMG_WIDTH\n",
        "    ylr.append(landmarks[2*start_idx:2*end_idx])\n",
        "\n",
        "  ylr = np.array(ylr)\n",
        "  Xlr = preprocess_input(X.astype(np.float32))\n",
        "\n",
        "  return (Xlr, ylr)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvvR14p9dqYS"
      },
      "source": [
        "## Create model functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axOcw4w_R9ek"
      },
      "source": [
        "def feature_extractor_base():\n",
        "    input_img = Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "\n",
        "    x = Conv2D(16,(3,3),padding=\"same\", kernel_initializer='he_uniform')(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3,3),strides=(2,2),padding=\"same\")(x)\n",
        "    \n",
        "    x = Conv2D(32,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3,3),strides=(2,2),padding=\"same\")(x)\n",
        "    \n",
        "    x = Conv2D(64,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3,3),strides=(2,2),padding=\"same\")(x)\n",
        "\n",
        "    x = Conv2D(128,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128,(3,3),padding=\"same\", kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3,3),strides=(2,2),padding=\"same\")(x)\n",
        "\n",
        "    output_vector = GlobalAvgPool2D()(x)\n",
        "\n",
        "    return Model(inputs = input_img, outputs=output_vector)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landmarks Model"
      ],
      "metadata": {
        "id": "bgA229CCnBbZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ7L5Glcgljy"
      },
      "source": [
        "def create_landmarks_model(output_size=136):\n",
        "  feature_extractor_base_model = feature_extractor_base()\n",
        "  feature_extractor_base_model.trainable = True\n",
        "  \n",
        "  input_img = Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "  feature_vector = feature_extractor_base_model(inputs=input_img)\n",
        "  dense_1 = Dense(units=128,kernel_initializer='glorot_uniform')(feature_vector)\n",
        "  lr_output = Dense(units=output_size,kernel_initializer='glorot_uniform',name=\"lr_output\")(dense_1)\n",
        "\n",
        "  return Model(inputs=input_img, outputs=lr_output)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVg7ox9-FXQA"
      },
      "source": [
        "### Emotion model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb8DapoxgeeD"
      },
      "source": [
        "def emotion_classifier_model(feature_extractor_base_model, num_classes = 7):\n",
        "\n",
        "  input_img = Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "  feature_vector = feature_extractor_base_model(input_img, training = False)\n",
        "  dense_1 = Dense(units=128,kernel_initializer='glorot_uniform')(feature_vector)\n",
        "  dense_2 = Dense(units=128,kernel_initializer='glorot_uniform')(dense_1)\n",
        "  emotion_output = Dense(units=num_classes, activation='softmax',name=\"emotion_output\",kernel_initializer='glorot_uniform')(dense_2)\n",
        "\n",
        "  return Model(inputs=input_img, outputs=emotion_output)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNge4TESIjKd"
      },
      "source": [
        "def create_emotion_classifier_model(landmarks_checkpoint = \"\"):\n",
        "  \n",
        "  ## set up feature extractor model\n",
        "  feature_extractor_base_model = feature_extractor_base()\n",
        "\n",
        "  landmarks_model = load_model(os.path.join(CHECKPOINT_PATH,landmarks_checkpoint))\n",
        "\n",
        "  feature_extractor_base_model_layers = feature_extractor_base_model.layers\n",
        "  feature_extractor_landmarks_submodel_layers = landmarks_model.layers[1].layers\n",
        "  \n",
        "  for i in range(len(feature_extractor_landmarks_submodel_layers)):\n",
        "    feature_extractor_base_model_layers[i].set_weights(feature_extractor_landmarks_submodel_layers[i].get_weights())\n",
        "\n",
        "  feature_extractor_base_model.trainable=False\n",
        "  emotion_model = emotion_classifier_model(feature_extractor_base_model)\n",
        "\n",
        "  emotion_model.summary()\n",
        "\n",
        "  return emotion_model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBh34xmw5zKZ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvV2bOIlJTaG"
      },
      "source": [
        "#### LR training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnbeAJRolZFd"
      },
      "source": [
        "lr_model_name = \"full\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvXEjts0jHZF"
      },
      "source": [
        "landmarks_dict = {\n",
        "    \"full\":{\n",
        "        \"start_idx\":0,\n",
        "        \"end_idx\":68,\n",
        "        \"output_size\":136\n",
        "    },\n",
        "    \"mouth\":{\n",
        "        \"start_idx\":48,\n",
        "        \"end_idx\":68,\n",
        "        \"output_size\":40\n",
        "    },\n",
        "    \"eyes\":{\n",
        "        \"start_idx\":36,\n",
        "        \"end_idx\":48,\n",
        "        \"output_size\":24\n",
        "    },\n",
        "    \"nose\":{\n",
        "        \"start_idx\":27,\n",
        "        \"end_idx\":36,\n",
        "        \"output_size\":18\n",
        "    },\n",
        "    \"eyebrows\":{\n",
        "        \"start_idx\":17,\n",
        "        \"end_idx\":27,\n",
        "        \"output_size\":20\n",
        "    },\n",
        "    \"jaw\":{\n",
        "        \"start_idx\":0,\n",
        "        \"end_idx\":17,\n",
        "        \"output_size\":34\n",
        "    }\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VgBLl2vuD48"
      },
      "source": [
        "learning_rate_base_reg_lr = 0.01\n",
        "batch_size_reg_lr = 32\n",
        "epochs_reg_lr = 10\n",
        "steps_per_epoch_reg_lr = 30\n",
        "checkpoint_name_reg_lr = f'checkpoint_landmarks_model_{lr_model_name}_test1'\n",
        "filename_reg_lr = f\"logs_dict_lr_{lr_model_name}_test1.json\"\n",
        "preprocess_input_reg_lr = preprocess_input_v1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL7KA9PJkaM1",
        "outputId": "7108b729-b8f4-40a9-d088-eab39906001b"
      },
      "source": [
        "aug_lr_train = ImageDataGeneratorLandmarksOnly(aug,ert_landmarks_predictor, gen_batch_size=1, preprocess_input = preprocess_input_reg_lr, start_idx = landmarks_dict[lr_model_name][\"start_idx\"], end_idx = landmarks_dict[lr_model_name][\"end_idx\"],verbose=0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'function'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFPCsoLhuD4_"
      },
      "source": [
        "## params dictionary\n",
        "params_reg_lr = {\n",
        "  \"output_size\":landmarks_dict[lr_model_name][\"output_size\"],\n",
        "  \"start_idx\":landmarks_dict[lr_model_name][\"start_idx\"],\n",
        "  \"end_idx\":landmarks_dict[lr_model_name][\"end_idx\"],\n",
        "  \"learning_rate_base\": learning_rate_base_reg_lr,\n",
        "  \"batch_size\":batch_size_reg_lr,\n",
        "  \"epochs\":epochs_reg_lr,\n",
        "  \"steps_per_epoch\":steps_per_epoch_reg_lr,\n",
        "  \"checkpoint\":checkpoint_name_reg_lr,\n",
        "  \"filename\":filename_reg_lr,\n",
        "  \"model_name\":lr_model_name,\n",
        "  \"preprocess_input\":preprocess_input_reg_lr,\n",
        "  \"aug_lr\":aug_lr_train,\n",
        "  \"landmarks_predictor\":ert_landmarks_predictor\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyZeugFcuD5E"
      },
      "source": [
        "def reg_training_lr(X_train, y_train, X_valid, y_valid, params):\n",
        "  \n",
        "  print(f\"------- Creating/Loading model-------\")\n",
        "  # use your own base checkpoint path refering to google drive directory where model will be saved\n",
        "  checkpoint_full_path = os.path.join(CHECKPOINT_PATH,params[\"checkpoint\"])\n",
        "  recovery_filepath = os.path.join(RECOVERY_DIR, params[\"filename\"])\n",
        "\n",
        "  ## optional load\n",
        "  try:\n",
        "    landmarks_model = load_model(os.path.join(CHECKPOINT_PATH,params[\"checkpoint\"]))\n",
        "    print(\"loaded model\")\n",
        "    landmarks_model.summary()\n",
        "  \n",
        "  except:\n",
        "    ## create emotion model\n",
        "    landmarks_model = create_landmarks_model(params[\"output_size\"])\n",
        "    print(\"new model\")  \n",
        "    landmarks_model.summary()\n",
        "    landmarks_model.compile(loss={\"lr_output\":\"mae\"},optimizer=Adam(learning_rate=params[\"learning_rate_base\"]))\n",
        "\n",
        "\n",
        "  #### Training landmarks model\n",
        "  monitor_training_callback = MonitorSingleTraining(filepath = recovery_filepath , model_name = params[\"model_name\"], epochs=params[\"epochs\"])\n",
        "  callbacks = [\n",
        "    ModelCheckpoint(filepath=checkpoint_full_path,\n",
        "      save_weights_only=False,\n",
        "      monitor='val_loss', \n",
        "      mode='min',\n",
        "      save_best_only=True,\n",
        "      verbose=1),\n",
        "      monitor_training_callback\n",
        "  ]\n",
        "\n",
        "  epochs_left = monitor_training_callback.get_epochs_left()\n",
        "\n",
        "  ## compile emotion model\n",
        "  landmarks_model.compile(loss={\"lr_output\":\"mae\"},optimizer=Adam(lr=params[\"learning_rate_base\"]))\n",
        "\n",
        "  ## Start training\n",
        "  print(f\"------- Training model -------\")\n",
        "\n",
        "  ## fit data\n",
        "  history = landmarks_model.fit(\n",
        "                      x = params[\"aug_lr\"].flow(X_train, y_train, \n",
        "                      batch_size = params[\"batch_size\"]),\n",
        "                      epochs= epochs_left, # do not change epochs here, you can set the value in params[\"epochs\"]\n",
        "                      validation_data = (X_valid, y_valid),\n",
        "                      steps_per_epoch=params[\"steps_per_epoch\"],\n",
        "                      callbacks = callbacks,\n",
        "                      verbose = 1)\n",
        "  \n",
        "  return history"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfUPSzK8QmE3"
      },
      "source": [
        "Xlr_valid, ylr_valid = create_test_dataset(np.array(valid_face_crop), params_reg_lr[\"landmarks_predictor\"], preprocess_input=params_reg_lr[\"preprocess_input\"], start_idx = params_reg_lr[\"start_idx\"], end_idx = params_reg_lr[\"end_idx\"])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYFC92rBR0HZ"
      },
      "source": [
        "history_lr = reg_training_lr(train_face_crop, y_train, Xlr_valid, ylr_valid, params=params_reg_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwKZsJgsR0Ha"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1jy4DvR0Hb"
      },
      "source": [
        "#### Emotion training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngEhnjVbR0Hb"
      },
      "source": [
        "lr_model_name = \"full\"\n",
        "model_name = \"lr\""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXB3YHi_R0Hb"
      },
      "source": [
        "learning_rate_base_reg_emotion = 0.01\n",
        "batch_size_reg_emotion = 32\n",
        "epochs_reg_emotion = 10\n",
        "steps_per_epoch_reg_emotion = 30\n",
        "checkpoint_name_reg_emotion = f'checkpoint_{model_name}_emotion_model_{lr_model_name}_test1'\n",
        "checkpoint_name_reg_lr = f'checkpoint_landmarks_model_{lr_model_name}_test1'\n",
        "filename_reg_emotion = f\"logs_dict_{model_name}_emotion_{lr_model_name}_test1.json\"\n",
        "preprocess_input_reg_emotion = preprocess_input_v1"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59hkUEDzR0Hb"
      },
      "source": [
        "aug_emotion_train = ImageDataGeneratorEmotionOnly(aug,ert_landmarks_predictor, gen_batch_size=1, verbose=0, preprocess_input=preprocess_input_reg_emotion)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmQ8OBhVR0Hb"
      },
      "source": [
        "## params dictionary\n",
        "params_reg_emotion = {\n",
        "  \"learning_rate_base\": learning_rate_base_reg_emotion,\n",
        "  \"batch_size\":batch_size_reg_emotion,\n",
        "  \"epochs\":epochs_reg_emotion,\n",
        "  \"steps_per_epoch\":steps_per_epoch_reg_emotion,\n",
        "  \"checkpoint\":checkpoint_name_reg_emotion,\n",
        "  \"checkpoint_lr\":checkpoint_name_reg_lr,\n",
        "  \"filename\":filename_reg_emotion,\n",
        "  \"model_name\":model_name,\n",
        "  \"preprocess_input\":preprocess_input_reg_emotion,\n",
        "  \"aug_emotion\":aug_emotion_train\n",
        "}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA5VI6c3R0Hc"
      },
      "source": [
        "def reg_training_emotion(X_train, y_train, X_valid, y_valid, params):\n",
        "  \n",
        "  print(f\"------- Creating/Loading model-------\")\n",
        "\n",
        "  # use your own base checkpoint path refering to google drive directory where model will be saved\n",
        "  checkpoint_full_path = os.path.join(CHECKPOINT_PATH,params[\"checkpoint\"])\n",
        "  checkpoint_lr_full_path = os.path.join(CHECKPOINT_PATH,params[\"checkpoint_lr\"])\n",
        "  recovery_filepath = os.path.join(RECOVERY_DIR, params[\"filename\"])\n",
        "\n",
        "  ## optional load\n",
        "  try:\n",
        "    emotion_model = load_model(checkpoint_full_path)\n",
        "    print(\"loaded model\")\n",
        "    emotion_model.summary()\n",
        "\n",
        "  except:\n",
        "    ## create emotion model\n",
        "    emotion_model = create_emotion_classifier_model(landmarks_checkpoint=checkpoint_lr_full_path)\n",
        "    print(\"new model\")\n",
        "    emotion_model.summary()\n",
        "    emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=params[\"learning_rate_base\"]), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  monitor_training_callback = MonitorSingleTraining(filepath = recovery_filepath , model_name = params[\"model_name\"], epochs=params[\"epochs\"])\n",
        "  callbacks = [\n",
        "    ModelCheckpoint(filepath=checkpoint_full_path,\n",
        "      save_weights_only=False,\n",
        "      monitor='val_loss', \n",
        "      mode='min',\n",
        "      save_best_only=True,\n",
        "      verbose=1),\n",
        "      monitor_training_callback\n",
        "  ]\n",
        "\n",
        "  epochs_left = monitor_training_callback.get_epochs_left()\n",
        "\n",
        "  ## Start training\n",
        "  print(f\"------- Training model -------\")\n",
        "\n",
        "  ## fit data\n",
        "  history = emotion_model.fit(\n",
        "                      x=params[\"aug_emotion\"].flow(X_train, y_train, batch_size=params[\"batch_size\"]), \n",
        "                      epochs= epochs_left, # do not change epochs here, you can set the value in params[\"epochs\"]\n",
        "                      validation_data = (X_valid, y_valid),\n",
        "                      steps_per_epoch=params[\"steps_per_epoch\"],\n",
        "                      callbacks = callbacks,\n",
        "                      verbose = 1)\n",
        "  \n",
        "  emotion_model=load_model(checkpoint_full_path)\n",
        "\n",
        "  ## generate generalization metrics\n",
        "  ## load model\n",
        "  scores = emotion_model.evaluate(X_valid,  y_valid, verbose=0)\n",
        "  print(f'Score for model {params[\"model_name\"]}: {emotion_model.metrics_names[0]} of {scores[0]}; {emotion_model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  return history\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRbUIJe_R0Hc"
      },
      "source": [
        "X_valid = params_reg_emotion[\"preprocess_input\"](np.array(valid_face_crop).astype(np.float32))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w50srHEAR0Hc"
      },
      "source": [
        "history_emotion = reg_training_emotion(train_face_crop, y_train, X_valid, y_valid, params=params_reg_emotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "towg-X9pvyk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-HwmOe2tvyrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workspace for testing ..."
      ],
      "metadata": {
        "id": "mbMiervmvzPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0zlhft0GvyuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}